{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62647611-a926-4bc3-8979-d07969fe92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Polynomial functions and kernel functions are both mathematical techniques used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods.\n",
    "\n",
    "#Polynomial Functions:\n",
    "#In machine learning, polynomial functions are often used as basis functions to transform the input features into a higher-dimensional space. This transformation can help make the data more separable, allowing linear classifiers to effectively separate the classes in the transformed space.\n",
    "\n",
    "#For example, consider a two-dimensional dataset that is not linearly separable in its original form. By applying a polynomial transformation, we can map the data into a higher-dimensional space where it becomes linearly separable. This is achieved by creating new features that are combinations of the original features, such as squares, products, and higher-order terms.\n",
    "\n",
    "#Kernel Functions:\n",
    "#Kernel functions, on the other hand, are used to implicitly perform the high-dimensional feature mapping without explicitly computing the transformed features. They measure the similarity or distance between pairs of data points in the original feature space. In SVMs, kernel functions are employed to define the inner products between the transformed feature vectors, which are crucial for decision making and classification.\n",
    "\n",
    "#A popular kernel function used in SVMs is the polynomial kernel. It calculates the similarity between two data points as the inner product of their transformed feature vectors in the higher-dimensional space induced by a polynomial transformation. This allows SVMs to effectively learn nonlinear decision boundaries in the original feature space without explicitly computing the transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab3c2dd-7b2c-4c91-9fb1-ab2c1e86ba59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "#2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "#Ans\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate example feature and label data\n",
    "features = np.random.randn(100, 2)\n",
    "labels = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Split the dataset into features (X) and labels (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the SVM classifier with a polynomial kernel\n",
    "svm_classifier = svm.SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96caded1-5d64-46cd-90ff-6f3890dabedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#In Support Vector Regression (SVR), the value of epsilon, denoted as ε, is a hyperparameter that controls the width of the epsilon-insensitive tube around the predicted function. The epsilon-insensitive tube is used to determine which data points are considered support vectors.\n",
    "\n",
    "#When you increase the value of epsilon, it allows more data points to fall within the epsilon-insensitive tube without contributing to the loss or error. Consequently, increasing epsilon tends to increase the number of support vectors in SVR.\n",
    "\n",
    "#Intuitively, as epsilon becomes larger, the model becomes more tolerant to errors, allowing a wider margin and accepting more data points within the tube as support vectors. This can lead to a more flexible or \"looser\" model that is less sensitive to individual data points.\n",
    "\n",
    "#However, it's worth noting that the relationship between the value of epsilon and the number of support vectors may not always be linear or straightforward. It can vary depending on the specific dataset, the complexity of the problem, and other hyperparameters involved in the SVR model. Therefore, it's important to perform empirical experiments and cross-validation to find an optimal value for epsilon that balances model flexibility and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40fefa6-0efe-485e-a448-54b687e84ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly affect the performance of the model. Let's discuss each parameter and its impact:\n",
    "\n",
    "#1 - Kernel Function:\n",
    "#The kernel function determines the type of function used to transform the input data into a higher-dimensional feature space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel depends on the nature of the data and the complexity of the problem. Here are a few examples:\n",
    "#Linear Kernel: Suitable for linearly separable data.\n",
    "#Polynomial Kernel: Effective for problems with complex, non-linear relationships.\n",
    "#RBF Kernel: Often a good choice when there is no prior knowledge about the data and for handling non-linear relationships.\n",
    "#Sigmoid Kernel: Useful for problems where the data can be mapped to a logistic regression-like problem.\n",
    "\n",
    "#2 - C Parameter:\n",
    "#The C parameter controls the trade-off between the model's complexity and the extent to which deviations from the true function are tolerated. It represents the regularization parameter in SVR. A higher value of C allows the model to fit the training data more precisely, potentially leading to overfitting. Conversely, a smaller value of C promotes a smoother solution and may increase generalization capability. Some guidelines for adjusting C:\n",
    "#Increase C when you want to prioritize fitting the training data accurately, especially if you suspect high noise or outliers.\n",
    "#Decrease C to increase the model's robustness against noise and outliers and promote smoother solutions.\n",
    "\n",
    "#3 - Epsilon Parameter:\n",
    "#The epsilon parameter (ε) defines the width of the epsilon-insensitive tube around the predicted function. It determines the threshold within which errors are not penalized, allowing them to fall within the tube. Larger epsilon values lead to a wider tube and a more tolerant model, while smaller epsilon values make the model less tolerant. Consider the following scenarios:\n",
    "#Increase epsilon when you expect larger errors to be acceptable or when you have noisy data.\n",
    "#Decrease epsilon if you want the model to be more sensitive to errors or if you have a small amount of noise.\n",
    "\n",
    "#4 - Gamma Parameter:\n",
    "#The gamma parameter (γ) determines the influence of each training sample on the decision boundary. It defines the size of the Gaussian radial basis function (RBF) kernel and controls the smoothness of the decision boundary. Higher gamma values lead to more complex decision boundaries that are sensitive to individual data points, potentially causing overfitting. Lower gamma values create smoother decision boundaries. Consider the following guidelines:\n",
    "#Increase gamma when you want the model to have more localized decision boundaries, especially in cases with more complex or nonlinear data.\n",
    "#Decrease gamma to create smoother decision boundaries, especially when dealing with high-dimensional data or a large number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e65ae77-58f2-424b-8d82-5713bb4252a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Parameters: {'C': 1, 'gamma': 0.1}\n",
      "Tuned Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#5. Assignment:\n",
    "\n",
    "#Import the necessary libraries and load the dataset\n",
    "#Split the dataset into training and testing sets\n",
    "#Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "#Create an instance of the SVC classifier and train it on the trai ng data\n",
    "#Use the trained classifier to predict the labels of the testing data\n",
    "#Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, Fl-score)\n",
    "#Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "#Train the tuned classifier on the entire dataset\n",
    "#Save the trained classifier to a file for future use.\n",
    "\n",
    "# I have used Iris dataset\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 7: Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Step 8: Train the tuned classifier on the training set with the best hyperparameters\n",
    "svm_classifier_tuned = svm.SVC(C=best_params['C'], gamma=best_params['gamma'])\n",
    "svm_classifier_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 9: Evaluate the performance of the tuned classifier on the testing set\n",
    "y_pred_tuned = svm_classifier_tuned.predict(X_test_scaled)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "print(\"Tuned Classifier Accuracy:\", accuracy_tuned)\n",
    "\n",
    "# Step 10: Save the trained tuned classifier to a file\n",
    "filename = 'svm_classifier_tuned.pkl'\n",
    "pickle.dump(svm_classifier_tuned, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02299743-db56-4945-a272-c9b4496a6d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
